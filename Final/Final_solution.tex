\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[left=1cm, right=1cm, top=2cm, bottom=2cm]{geometry}
\title{\textbf{COM 5232 Detection and Estimation Theory}}
\author{\textbf{Final Exam Solution}}
\date{June 7, 2022\\
13:20 $\sim$ 15:10
}
\begin{document}
    \maketitle
    \begin{enumerate}
        \item
            \begin{enumerate}
                \item Yes
                    \begin{align*}
                    \hat\sigma^2 &=\frac{1}{N}\sum\limits_{n=0}^{N-1}x^2[n]\\
                    \Longrightarrow\ \mathbb E[\hat\sigma^2]&=\frac{1}{N}\sum\limits_{n=0}^{N-1}\mathbb E\left[x^2[n]\right]\\
                    &=\frac{1}{N}\sum\limits_{n=0}^{N-1}\sigma^2\\
                    &=\sigma^2
                    \end{align*}
                    Thus, $\hat\sigma^2$ is an unbiased estimator.
                \item
                    \begin{align*}
                        Var(\hat\sigma^2) &= \mathbb E\left[(\hat\sigma^2)^2\right]- \left(\mathbb E\left[\hat\sigma^2\right]\right)^2\\
                        &= \mathbb E\left[\hat\sigma^4\right]-\sigma^4\\
                        &= \mathbb E\left[\frac{1}{N^2}\sum\limits_{n=0}^{N-1}\sum\limits_{\substack{m=0 \\ m \neq n}}^{N-1}x^2[n]x^2[m]\right]+\mathbb E\left[\frac{1}{N^2}\sum\limits_{n=0}^{N-1}x^4[n]\right]-\sigma^4\\
                        ({\rm since}\ x[n]{\rm 's}\ {\rm are\ IID})&= \frac{1}{N^2}\sum\limits_{n=0}^{N-1}\sum\limits_{\substack{m=0 \\ m \neq n}}^{N-1}\mathbb E\left[x^2[n]\right]\mathbb E\left[x^2[m]\right]+\frac{1}{N^2}\sum\limits_{n=0}^{N-1}\underbrace{\mathbb E\left[x^4[n]\right]}_{3\sigma^4}-\sigma^4\\
                        &=\frac{2}{N}\sigma^4
                    \end{align*}
            \end{enumerate}
            \begin{flushright}
                $\blacksquare$
            \end{flushright}
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item
            \begin{align*}
                p(\mathbf x;A) &=\frac{1}{(2\pi)^\frac{N}{2}\det{(\mathbf C)}^{\frac{-1}{2}}}\exp{\left(\frac{-1}{2}(\mathbf x-A\mathbf 1)^T\mathbf C^{-1}(\mathbf x-A\mathbf 1)\right)}\\
                \Longrightarrow\frac{\partial\ln{p(\mathbf x;A)}}{\partial A} &= \mathbf 1^T\mathbf C^{-1}\mathbf x-\mathbf 1^T\mathbf C^{-1}\mathbf 1A\\
                \Longrightarrow\frac{\partial^2\ln{p(\mathbf x;A)}}{\partial A^2} &=-\mathbf 1^T\mathbf C^{-1}\mathbf 1\\
            \end{align*}
            Thus the CRLB for $A$ is $\frac{1}{\mathbf 1^T\mathbf C^{-1}\mathbf 1}$
            \begin{flushright}
                $\blacksquare$
            \end{flushright}
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item
            Let
            \begin{align*}
                &\mathbf x=[x[0]\ x[1]\ \dots\ x[N-1]]^T\\
                &\mathbf w=[w[0]\ w[1]\ \dots\ w[N-1]]^T\\
                &\mathbf H=\begin{bmatrix}
                r_1^0       &   r_2^0       &   r_3^0\\
                r_1^1       &   r_2^1       &   r_3^1\\
                \vdots      &   \vdots      &   \vdots\\
                r_1^{N-1}   &   r_2^{N-1}   &   r_3^{N-1}\end{bmatrix},\ \mathbf A=\begin{bmatrix}A_1\\A_2\\A_3\end{bmatrix}
            \end{align*}
            Then we have
            \begin{align*}
                \mathbf x =\mathbf H\mathbf A+\mathbf w
            \end{align*}
            and the MVU estimator is given by
            \begin{align*}
                \hat{\mathbf A}=(\mathbf H^T\mathbf H)^{-1}\mathbf H^T\mathbf x
            \end{align*}
            For $r_1=1,r_2=-1,r_3=2$
            \begin{align*}
                \mathbf H^T\mathbf H&=\begin{bmatrix}
                N       &   0       &   2^N-1\\
                0       &   N       &   \frac{1-(-2)^N}{3}\\
                2^N-1   &   \frac{1-(-2)^N}{3}  &   \frac{4^N-1}{3}
                \end{bmatrix}\\
                \det{(\mathbf H^T\mathbf H)}&=\frac{N^2(4^N-1)}{3}-N(2^N-1)^2-\frac{N(1-2^N)^2}{9}\\
                \Longrightarrow (\mathbf H^T\mathbf H)^{-1} &=\frac{1}{\det{(\mathbf H^T\mathbf H)}}\begin{bmatrix}
                \frac{N}{3}(4^N-1)-\frac{1}{9}(2^N-1)^2     &   -\frac{1}{3}(2^N-1)^2           &   -N(2^N-1)\\
                -\frac{1}{3}(2^N-1)^2                       &   \frac{N}{3}(4^N-1)-(2^N-1)^2    &   \frac{N}{3}(2^N-1)\\
                -N(2^N-1)                                   &   \frac{N}{3}(2^N-1)              &   N^2
                \end{bmatrix}\\
                (N=4)
                &=\begin{bmatrix}
                \frac{7}{8}     &   -\frac{5}{24}   &   -\frac{1}{6}\\
                -\frac{5}{24}   &   \frac{23}{72}   &   \frac{1}{18}\\
                -\frac{1}{6}    &   \frac{1}{18}    &   \frac{2}{45}
                \end{bmatrix}
            \end{align*}
            Thus, we derive the MVU estimator
            \begin{align*}
                \hat{\mathbf A}&=\begin{bmatrix}
                \frac{7}{8}     &   -\frac{5}{24}   &   -\frac{1}{6}\\
                -\frac{5}{24}   &   \frac{23}{72}   &   \frac{1}{18}\\
                -\frac{1}{6}    &   \frac{1}{18}    &   \frac{2}{45}
                \end{bmatrix}\begin{bmatrix}
                1   &   1   &   1   &   1\\
                1   &   -1  &   1   &   -1\\
                1   &   2   &   4   &   8
                \end{bmatrix}\begin{bmatrix}x[0]\\x[1]\\x[2]\\x[3]\end{bmatrix}\\
                &=\begin{bmatrix}
                \frac{1}{2}     &  \frac{3}{4}      &   0               &   -\frac{1}{4}\\
                \frac{1}{6}     &   -\frac{5}{12}   &   \frac{1}{3}     &   -\frac{1}{12}\\
                -\frac{1}{15}   &   -\frac{2}{15}   &   \frac{1}{15}    &   \frac{2}{15}
                \end{bmatrix}\begin{bmatrix}x[0]\\x[1]\\x[2]\\x[3]\end{bmatrix}
            \end{align*}
            \begin{flushright}
                $\blacksquare$
            \end{flushright}
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item
            \begin{align*}
                \mathbf x=\mathbf H\bm\theta+\mathbf s+\mathbf w
            \end{align*}
            Since $\mathbf s$ is known, consider
            \begin{align*}
                \mathbf y &=\mathbf x-\mathbf s\\
                &=\mathbf H\bm\theta+\mathbf w
            \end{align*}
            Then the BLUE is given by
            \begin{align*}
                \hat{\bm\theta} &=(\mathbf H^T\mathbf C^{-1}\mathbf H)^{-1}\mathbf H^T\mathbf C^{-1}\mathbf y\\
                &=(\mathbf H^T\mathbf C^{-1}\mathbf H)^{-1}\mathbf H^T\mathbf C^{-1}(\mathbf x-\mathbf s)
            \end{align*}
            \begin{flushright}
                $\blacksquare$
            \end{flushright}
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item
            \begin{enumerate}
                \item
                    \begin{align*}
                        \hat{\bm\theta}&=(\mathbf H^T\mathbf H)^{-1}\mathbf H^T\mathbf x\\
                        \mathbf x&\sim\mathcal N(\mathbf H\bm\theta,\sigma^2\mathbf I)
                    \end{align*}
                    Then
                    \begin{align*}
                        \mathbb E\left[\hat{\bm\theta}\right]&=(\mathbf H^T\mathbf H)^{-1}\mathbf H^T\mathbb E\left[\mathbf x\right]\\
                        &=\bm\theta\\
                        Var(\hat{\bm\theta}) &=\mathbb E\left[\hat{\bm\theta}\hat{\bm\theta}^T\right]-\bm\theta\bm\theta^T\\
                        &=(\mathbf H^T\mathbf H)^{-1}\mathbf H^T\mathbb E\left[\mathbf x\mathbf x^T\right]\mathbf H(\mathbf H^T\mathbf H)^{-1}-\bm\theta\bm\theta^T\\
                        &=(\mathbf H^T\mathbf H)^{-1}\mathbf H^T(\sigma^2\mathbf I+\mathbf H\bm\theta\bm\theta^T\mathbf H^T)\mathbf H(\mathbf H^T\mathbf H)^{-1}-\bm\theta\bm\theta^T\\
                        &=\sigma^2(\mathbf H^T\mathbf H)^{-1}
                    \end{align*}
                    Thus $\hat{\bm\theta}\sim\mathcal N(\bm\theta,\sigma^2(\mathbf H^T\mathbf H)^{-1})$
                    \item Yes, it has been verified in (a).
            \end{enumerate}
            \begin{flushright}
                $\blacksquare$
            \end{flushright}
    \end{enumerate}
    \rule{\textwidth}{0.4pt}
\end{document}
